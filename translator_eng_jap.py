# -*- coding: utf-8 -*-
"""translator eng-jap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X27tFydPntcEXcEGjz_NtZD7g10Gb4C3
"""

!pip install wget
!pip install tensorflow

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d team-ai/japaneseenglish-bilingual-corpus

!kaggle datasets download -d nexdatafrank/english-japanese-parallel-corpus-data

import zipfile

# Unzip the first dataset
with zipfile.ZipFile('japaneseenglish-bilingual-corpus.zip', 'r') as zip_ref:
    zip_ref.extractall('japaneseenglish-bilingual-corpus')

# Unzip the second dataset
with zipfile.ZipFile('english-japanese-parallel-corpus-data.zip', 'r') as zip_ref:
    zip_ref.extractall('english-japanese-parallel-corpus')

import os

# List files in the first dataset directory
print("Files in 'japaneseenglish-bilingual-corpus':")
print(os.listdir('japaneseenglish-bilingual-corpus'))

# List files in the second dataset directory
print("Files in 'english-japanese-parallel-corpus':")
print(os.listdir('english-japanese-parallel-corpus'))

# Ensure we have the correct files listed
import os

# List files in the main directory
print("Files in 'japaneseenglish-bilingual-corpus':")
print(os.listdir('japaneseenglish-bilingual-corpus'))

# Load kyoto_lexicon.csv, skipping bad lines
import pandas as pd

kyoto_lexicon_path = 'japaneseenglish-bilingual-corpus/kyoto_lexicon.csv'
kyoto_lexicon = pd.read_csv(kyoto_lexicon_path, on_bad_lines='skip')
print("First few rows of kyoto_lexicon.csv:")
print(kyoto_lexicon.head())

# Extract wiki_corpus_2.01.tar
import tarfile

tar_path = 'japaneseenglish-bilingual-corpus/wiki_corpus_2.01.tar'
tar = tarfile.open(tar_path)
tar.extractall(path='japaneseenglish-bilingual-corpus/wiki_corpus_2.01')
tar.close()

# List the contents of the extracted directory
print("Extracted files in 'wiki_corpus_2.01':")
print(os.listdir('japaneseenglish-bilingual-corpus/wiki_corpus_2.01'))

# Load Wiki_Corpus_List_2.01.csv
wiki_corpus_path = 'japaneseenglish-bilingual-corpus/Wiki_Corpus_List_2.01.csv'
wiki_corpus = pd.read_csv(wiki_corpus_path)
print("First few rows of Wiki_Corpus_List_2.01.csv:")
print(wiki_corpus.head())

# Inspect the column names of wiki_corpus
print(wiki_corpus.columns)

import os

directory_path = 'japaneseenglish-bilingual-corpus/wiki_corpus_2.01/SNT/'
print(os.listdir(directory_path))
import os

directory_path = 'japaneseenglish-bilingual-corpus/wiki_corpus_2.01/SNT/'
print(os.listdir(directory_path))

import pandas as pd
from pathlib import Path

# Example: Adjust path based on where the file actually exists
base_path = 'japaneseenglish-bilingual-corpus/wiki_corpus_2.01/SNT/'
file_name = 'sentences.csv'
sentence_file_path = Path(base_path) / file_name

# Check if the file exists
if sentence_file_path.exists():
    sentences = pd.read_csv(sentence_file_path, delimiter='\t', header=None, names=['EN', 'JA'])
    print("First few rows of the sentence pairs:")
    print(sentences.head())
else:
    print(f"File '{file_name}' not found in '{base_path}'. Please check the path.")

import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Define the path to the CSV file containing sentence pairs
csv_file_path = 'japaneseenglish-bilingual-corpus/wiki_corpus_2.01/Wiki_Corpus_List_2.01.csv'

# Load the CSV file
sentences = pd.read_csv(csv_file_path, delimiter='\t', header=None, names=['EN', 'JA'])

# Drop rows with NaN values (if any) in either 'EN' or 'JA' columns
sentences.dropna(subset=['EN', 'JA'], inplace=True)

print("First few rows of the sentence pairs:")
print(sentences.head())

# Assuming you have a preprocessing function defined as preprocess_sentence
def preprocess_sentence(sentence):
    # Implement your preprocessing logic here
    return sentence

# Preprocess and tokenize the sentences
sentences['EN'] = sentences['EN'].apply(preprocess_sentence)
sentences['JA'] = sentences['JA'].apply(lambda x: preprocess_sentence(str(x)))  # Convert NaN to string for preprocessing

# Tokenize the English sentences
eng_tokenizer = Tokenizer()
eng_tokenizer.fit_on_texts(sentences['EN'])
eng_sequences = eng_tokenizer.texts_to_sequences(sentences['EN'])

# Tokenize the Japanese sentences
jpn_tokenizer = Tokenizer()
jpn_tokenizer.fit_on_texts(sentences['JA'])
jpn_sequences = jpn_tokenizer.texts_to_sequences(sentences['JA'])

# Pad the sequences to the same length
max_len = 100  # Adjust as per your requirement
eng_padded = pad_sequences(eng_sequences, padding='post', maxlen=max_len)
jpn_padded = pad_sequences(jpn_sequences, padding='post', maxlen=max_len)

# Display the first few tokenized sequences
print("Tokenized English sequences:")
print(eng_padded[:5])
print("Tokenized Japanese sequences:")
print(jpn_padded[:5])

# Now you can proceed with defining and training your model
# Remember to adjust the tokenization and padding parameters based on your specific requirements

# Tokenize the English sentences
eng_tokenizer = Tokenizer()
eng_tokenizer.fit_on_texts(sentences['EN'])
eng_sequences = eng_tokenizer.texts_to_sequences(sentences['EN'])

# Tokenize the Japanese sentences
jpn_tokenizer = Tokenizer()
jpn_tokenizer.fit_on_texts(sentences['JA'])
jpn_sequences = jpn_tokenizer.texts_to_sequences(sentences['JA'])

# Pad the sequences to the same length
max_len = 100  # Adjust as per your requirement
eng_padded = pad_sequences(eng_sequences, padding='post', maxlen=max_len)
jpn_padded = pad_sequences(jpn_sequences, padding='post', maxlen=max_len)

# Display the first few tokenized sequences
print("Tokenized English sequences:")
print(eng_padded[:5])
print("Tokenized Japanese sequences:")
print(jpn_padded[:5])

print("Shape of eng_padded:", eng_padded.shape)
print("Shape of jpn_padded:", jpn_padded.shape)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

# Define the input and embedding layers for the encoder
encoder_inputs = Input(shape=(None,))
encoder_embedding = Embedding(input_dim=len(eng_tokenizer.word_index)+1, output_dim=256)(encoder_inputs)
encoder_lstm = LSTM(256, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]

# Define the input and embedding layers for the decoder
decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(input_dim=len(jpn_tokenizer.word_index)+1, output_dim=256)(decoder_inputs)
decoder_lstm = LSTM(256, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(len(jpn_tokenizer.word_index)+1, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Display the model summary
model.summary()
# Ensure your data has sufficient samples
print(f"Number of samples: {eng_padded.shape[0]}")

import pandas as pd

# Define the path to your CSV file
csv_file_path = 'japaneseenglish-bilingual-corpus/wiki_corpus_2.01/Wiki_Corpus_List_2.01.csv'

# Read the first few rows of the CSV file to inspect
data = pd.read_csv(csv_file_path, nrows=5)
print(data)

# Print the columns to verify their names
print(data.columns)

import pandas as pd

# Sample dataset
data = pd.DataFrame({
    'EN': [
        'This is a pen.',
        'I am a student.',
        'He is a teacher.',
        'She is reading a book.',
        'They are playing soccer.'
    ],
    'JA': [
        'これはペンです。',
        '私は学生です。',
        '彼は先生です。',
        '彼女は本を読んでいます。',
        '彼らはサッカーをしています。'
    ]
})

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense

# Preprocess and tokenize the sentences
def preprocess_sentence(sentence):
    return sentence.lower()

data['EN'] = data['EN'].apply(preprocess_sentence)
data['JA'] = data['JA'].apply(preprocess_sentence)

# Tokenize the English sentences
eng_tokenizer = Tokenizer()
eng_tokenizer.fit_on_texts(data['EN'])
eng_sequences = eng_tokenizer.texts_to_sequences(data['EN'])

# Tokenize the Japanese sentences
jpn_tokenizer = Tokenizer()
jpn_tokenizer.fit_on_texts(data['JA'])
jpn_sequences = jpn_tokenizer.texts_to_sequences(data['JA'])

# Pad the sequences to the same length
eng_padded = pad_sequences(eng_sequences, padding='post')
jpn_padded = pad_sequences(jpn_sequences, padding='post')

# Ensure padding does not create empty arrays
if eng_padded.shape[0] == 0 or jpn_padded.shape[0] == 0:
    raise ValueError("Padded sequences are empty. Check your data preprocessing steps.")

# Split data into training and validation sets
eng_train, eng_val, jpn_train, jpn_val = train_test_split(eng_padded, jpn_padded, test_size=0.2, random_state=42)

# Define maximum sequence length (adjust as per your data)
max_len = eng_padded.shape[1]  # Assuming eng_padded and jpn_padded have the same shape

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense

# Sample dataset
data = pd.DataFrame({
    'EN': [
        'This is a pen.',
        'I am a student.',
        'He is a teacher.',
        'She is reading a book.',
        'They are playing soccer.'
    ],
    'JA': [
        'これはペンです。',
        '私は学生です。',
        '彼は先生です。',
        '彼女は本を読んでいます。',
        '彼らはサッカーをしています。'
    ]
})

# Preprocess and tokenize the sentences
def preprocess_sentence(sentence):
    return sentence.lower()

data['EN'] = data['EN'].apply(preprocess_sentence)
data['JA'] = data['JA'].apply(preprocess_sentence)

# Tokenize the English sentences
eng_tokenizer = Tokenizer()
eng_tokenizer.fit_on_texts(data['EN'])
eng_sequences = eng_tokenizer.texts_to_sequences(data['EN'])

# Tokenize the Japanese sentences
jpn_tokenizer = Tokenizer()
jpn_tokenizer.fit_on_texts(data['JA'])
jpn_sequences = jpn_tokenizer.texts_to_sequences(data['JA'])

# Set a reasonable padding length
max_len = max(max(len(seq) for seq in eng_sequences), max(len(seq) for seq in jpn_sequences))

# Pad the sequences to the same length
eng_padded = pad_sequences(eng_sequences, maxlen=max_len, padding='post')
jpn_padded = pad_sequences(jpn_sequences, maxlen=max_len, padding='post')

# Check padded sequences
print("English sequences after padding:", eng_padded)
print("Japanese sequences after padding:", jpn_padded)

# Split data into training and validation sets
eng_train, eng_val, jpn_train, jpn_val = train_test_split(eng_padded, jpn_padded, test_size=0.2, random_state=42)

# Check the shapes of the split data
print("eng_train shape:", eng_train.shape)
print("eng_val shape:", eng_val.shape)
print("jpn_train shape:", jpn_train.shape)
print("jpn_val shape:", jpn_val.shape)

# Ensure sequences are long enough for slicing
assert eng_train.shape[1] > 1, "Sequences are too short to be split into input/target pairs."
assert jpn_train.shape[1] > 1, "Sequences are too short to be split into input/target pairs."

# Define input sequences
encoder_inputs = Input(shape=(max_len,))
decoder_inputs = Input(shape=(max_len-1,))

# Embedding layer for encoder
encoder_embedding = Embedding(input_dim=len(eng_tokenizer.word_index) + 1, output_dim=256)(encoder_inputs)
# Encoder LSTM
encoder_lstm, state_h, state_c = LSTM(256, return_state=True)(encoder_embedding)

# Embedding layer for decoder
decoder_embedding = Embedding(input_dim=len(jpn_tokenizer.word_index) + 1, output_dim=256)(decoder_inputs)
# Decoder LSTM
decoder_lstm = LSTM(256, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])

# Dense layer for prediction
decoder_dense = Dense(len(jpn_tokenizer.word_index) + 1, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Display the model summary
model.summary()

# Train the model
history = model.fit(
    [eng_train, jpn_train[:, :-1]],  # Input: English sequences and Japanese input sequences
    np.expand_dims(jpn_train[:, 1:], -1),  # Output: Japanese target sequences
    batch_size=64,
    epochs=10,
    validation_data=([eng_val, jpn_val[:, :-1]], np.expand_dims(jpn_val[:, 1:], -1))
)

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, Dense, LayerNormalization, Dropout, Add

# Preprocess and tokenize the sentences
def preprocess_sentence(sentence):
    return sentence.lower()

# Sample dataset
data = pd.DataFrame({
    'EN': [
        'This is a pen.',
        'I am a student.',
        'He is a teacher.',
        'She is reading a book.',
        'They are playing soccer.'
    ],
    'JA': [
        'これはペンです。',
        '私は学生です。',
        '彼は先生です。',
        '彼女は本を読んでいます。',
        '彼らはサッカーをしています。'
    ]
})

data['EN'] = data['EN'].apply(preprocess_sentence)
data['JA'] = data['JA'].apply(preprocess_sentence)

# Tokenize the English sentences
eng_tokenizer = Tokenizer()
eng_tokenizer.fit_on_texts(data['EN'])
eng_sequences = eng_tokenizer.texts_to_sequences(data['EN'])

# Tokenize the Japanese sentences
jpn_tokenizer = Tokenizer()
jpn_tokenizer.fit_on_texts(data['JA'])
jpn_sequences = jpn_tokenizer.texts_to_sequences(data['JA'])

# Set a reasonable padding length
max_len = max(max(len(seq) for seq in eng_sequences), max(len(seq) for seq in jpn_sequences))

# Pad the sequences to the same length
eng_padded = pad_sequences(eng_sequences, maxlen=max_len, padding='post')
jpn_padded = pad_sequences(jpn_sequences, maxlen=max_len, padding='post')

# Split data into training and validation sets
eng_train, eng_val, jpn_train, jpn_val = train_test_split(eng_padded, jpn_padded, test_size=0.2, random_state=42)

# Ensure sequences are long enough for slicing
assert eng_train.shape[1] > 1, "Sequences are too short to be split into input/target pairs."
assert jpn_train.shape[1] > 1, "Sequences are too short to be split into input/target pairs."

# Define Transformer Encoder Layer
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)
    x = Dropout(dropout)(x)
    x = Add()([x, inputs])
    x = LayerNormalization(epsilon=1e-6)(x)

    x_ff = Dense(ff_dim, activation="relu")(x)
    x_ff = Dropout(dropout)(x_ff)
    x_ff = Dense(inputs.shape[-1])(x_ff)
    x = Add()([x, x_ff])
    x = LayerNormalization(epsilon=1e-6)(x)
    return x

# Define Transformer Decoder Layer
def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout=0):
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)
    x = Dropout(dropout)(x)
    x = Add()([x, inputs])
    x = LayerNormalization(epsilon=1e-6)(x)

    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, encoder_outputs)
    x = Dropout(dropout)(x)
    x = Add()([x, inputs])
    x = LayerNormalization(epsilon=1e-6)(x)

    x_ff = Dense(ff_dim, activation="relu")(x)
    x_ff = Dropout(dropout)(x_ff)
    x_ff = Dense(inputs.shape[-1])(x_ff)
    x = Add()([x, x_ff])
    x = LayerNormalization(epsilon=1e-6)(x)
    return x

# Define model inputs
encoder_inputs = Input(shape=(max_len,))
decoder_inputs = Input(shape=(max_len-1,))

# Embedding layers
encoder_embedding = Embedding(input_dim=len(eng_tokenizer.word_index) + 1, output_dim=256)(encoder_inputs)
decoder_embedding = Embedding(input_dim=len(jpn_tokenizer.word_index) + 1, output_dim=256)(decoder_inputs)

# Encoder
encoder_outputs = transformer_encoder(encoder_embedding, head_size=256, num_heads=4, ff_dim=256, dropout=0.1)

# Decoder
decoder_outputs = transformer_decoder(decoder_embedding, encoder_outputs, head_size=256, num_heads=4, ff_dim=256, dropout=0.1)

# Final dense layer
decoder_dense = Dense(len(jpn_tokenizer.word_index) + 1, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Display the model summary
model.summary()

# Train the model
history = model.fit(
    [eng_train, jpn_train[:, :-1]],  # Input: English sequences and Japanese input sequences
    np.expand_dims(jpn_train[:, 1:], -1),  # Output: Japanese target sequences
    batch_size=64,
    epochs=10,
    validation_data=([eng_val, jpn_val[:, :-1]], np.expand_dims(jpn_val[:, 1:], -1))
)

# Display training history
import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.show()